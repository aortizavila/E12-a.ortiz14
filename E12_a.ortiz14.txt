Gradient Boosting es también un algoritmo de impulso, por lo tanto, también intenta crear un aprendiz fuerte a partir de un conjunto de aprendices débiles. Este algoritmo es similar a Adaptive Boosting (AdaBoost) pero difiere de él en ciertos aspectos. XGBoost es un algoritmo que ha estado dominando recientemente las competencias de aprendizaje automático para datos estructurados o tabulares. XGBoost es una implementación de árboles de decisión mejorados con gradiente diseñados para la velocidad y el rendimiento. La implementación del algoritmo fue diseñada para la eficiencia del tiempo de cómputo y los recursos de memoria. Un objetivo de diseño era hacer el mejor uso de los recursos disponibles para entrenar el modelo.
La idea es optimizar la función de costo mediante una función iterativa de una función que apunta en dirección del gradiente negativo.Gradient Boosted trees: Es un caso particular del Gradient Boosting, la funcion que predice, es decir los aprendizajes debiles, son arboles de decision. Como consecuencia, los par‡metros de entrada del algortimo son los mismo de un arbol de decision. XGBoost: Es un Gradient Boosted Trees ajustado para ser mas eficiente porque no se considera la perdida potencial de todos los posibles splits para crear una nueva rama. También se realizan procesos de estandarizacion, por lo que  la cantidad de hiperparametros aumenta pero el overfitting disminuye, algunos de los mas determinantes son n_estimators: Numero de arboles a ensamblar max_depth: Maxima profundidad de cada arbol learningÊrate: cada peso en todos los arboles se multiplica por este valor. 



